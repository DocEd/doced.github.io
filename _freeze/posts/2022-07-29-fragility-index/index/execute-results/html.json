{
  "hash": "72d631605c48f4c38307d0a1dd417f8d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Randomised Controlled Trials: Anything But Fragile\"\ndate: \"2022-07-29\"\ncsl: ./standard.csl\nauthor:\n  - name: Ed Palmer\n    url: https://doced.github.io/\n    orcid: 0000-0002-8024-4616\nexecute:\n  echo: false\nbibliography:\n  - ./fi.bib\n  - ./packages.bib\ncategories:\n  - statistics\neditor_options: \n  chunk_output_type: console\nfig-cap-location: margin\n---\n\n\n\n# Preamble\n\nI wrote this editorial some years ago in response to the deluge of reports that reappraise prior studies with the fragility index and the subsequent permeation of this method into the medical community.\nSadly, I could not get the article picked up by a medical journal (most suggested a methods journal instead, where this information is *already* known), and so I am posting it here.\nI hope this is of use to the medical community who currently use the fragility index.\n\n# Introduction\n\nIn 2014, the fragility index was proposed to evaluate the strength of evidence provided by a randomised controlled trial (RCT) [@walsh2014].\nThe method entails hypothetically moving patients, one at a time, within the intervention arm of an RCT from \"survival\" to \"death\" (@fig-fi).\nFor each patient move, the p value for the primary outcome is recalculated using Fisher’s exact test [@agresti2002].\nThe number of patients moved in order to render the p value greater than 0.05 is the fragility index.\n\n![Illustration of the calculation of the fragility index](./figure1.png){#fig-fi}\n\nThe originators of the fragility index applied it to nearly 400 high profile RCTs.\nThey contended that a large number of these RCTs were “fragile” in that a small number of patients experiencing a different outcome would change the conclusions of the study [@walsh2014].\nMany RCTs have now been evaluated with the fragility index; including studies in critical care; peri-operative medicine; surgery; anaesthetics; urology; ophthalmology; cardiology and nephrology [@ridgeon2016; @mazzinari2018; @bertaggia2019; @tignanelli2019; @shen2018; @narayan2018; @noel2018; @khan2017; @shochet2017; @docherty2016; @evaniew2015].\nMost RCTs, including those considered to be of a robust nature, have now been labelled as \"fragile\" by this approach.\n\nThe fragility index certainly has superficial appeal as it feels intuitive.\nHowever, I will demonstrate empirically with \"in-silico\" simulation that it is a simple transformation of the study's p value.\nThus, the fragility index cannot comment upon the validity of an RCT as no new information has been learnt.\nThe fragility index beguiles the reader into mistrusting the results of an otherwise well conducted study.\nIn this instance, we should ignore intuition, and instead rely upon the rigorous statistical frameworks that already have a rich history of reliably discriminating signal from noise.\n\nRCTs are powered a priori to detect a minimum clinically important difference (MCID); the smallest effect size that would be clinically meaningful.\nThis power calculation specifies the minimum number of patients to be recruited so that if a true effect exists (greater than or equal to the MCID), it will produce a \"significant\" result with a p value below the alpha boundary (typically set at 0.05) at a known frequency (typically 80-90%).\nOver-recruitment exposes more patients to potential harm and requires more resources and time.\n \nThe p value is a hypothetical frequency probability in that its properties are guaranteed in the long run over an infinite number of imagined future trials.\nIn practice, only a single RCT can be performed, and a single p value observed.\nI performed computer simulations of parallel arm RCTs to observe the long running behaviour of the p value and fragility index.\nNine scenarios were examined: a true absolute risk reduction (ARR) in mortality of 15%, 10% and 5%, each powered at 70%, 80% and 90%.\nControl group mortality was fixed at 30%.\nThe numbers of patients required for each combination of ARR and power ranged from 190 (15% ARR at 70% power) to 3,346 (5% ARR at 90% power).\nEach scenario was simulated 1,000 times.\nSimulations were conducted in R [@R-base] with code published in the public domain [@palmer2019a].\n\nFigure 2 shows the distribution of the fragility index for each scenario.\nFor a given effect size, the fragility index increases as the study power (and hence sample size) is increased.\nLikewise, for a given power, the fragility index increases as the effect size decreases (and hence sample size increases).\nCrucially, all the simulations are equally valid since the underlying data generating mechanism has been controlled.\nThe fragility index is thus an expression of the study characteristics, and not of validity.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![*Box and Whisker plots demonstrating the distribution of fragility index over a range of possible study scenarios. Each facet details 1000 realisations of the simulation. Levels of absolute risk reduction and power are organised by column and row respectively.*](./figure2.png){width=864}\n:::\n:::\n\n\n\nFigure 3 shows the clear relationship between the fragility index and the p value.\nThe fragility index can be regarded as a transformation of the p value, with the shape of the relationship determined by the number of participants, the observed treatment effect and control group mortality.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![*Relationship between p value and fragility index stratified by absolute risk reduction.*](./figure3.png){width=864}\n:::\n:::\n\n\n\nI found a near-perfect relationship (R^2^ = 0.99) between the fragility index and study p value (log transformed), sample size, observed treatment effect and observed control group mortality (figure 4).\nElements of this relationship have previously been recognised [@carter2016].\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![*Relationship between predicted fragility index and calculated fragility index. The predicted fragililty is a function of the log transformed p value, number of study participants, observed treatment effect and control group mortality.*](./figure4.png){width=864}\n:::\n:::\n\n\n\nThus, the fragility index is an encapsulation of these study parameters into a single figure.\nRCTs with identical p values (for example 0.04) will differ in their fragility index only due to the difference in these observed study characteristics.\n\nThere has been a long-standing tension between statistical analysis and the need to inform policy.\nThe former aims to describe an observed effect in terms of uncertainty, whereas policy (such as \"should I administer an antibiotic\") is necessarily binary, i.e. to act or not.\nUltimately, a line must be drawn where evidence is strong enough to prompt action.\nThat strength of evidence is commonly---though completely arbitrarily---set at a p value of 0.05.\nThe fragility index begs us to entertain a secondary decision boundary.\nAt what fragility index is a study now deemed to be no longer fragile?\n5, 10, 500?\nAs demonstrated above, this is simply a request for greater power to provide more evidence for the study question.\nIn RCTs, the optimal study size for a given effect size has already been determined a priori.\nThe Neyman-Pearson Lemma (a mathematical proof) demonstrates that any secondary decision boundary is necessarily less optimal than the original [@rice2014].\n\nThe impact of the fragility index has a lasting negative effect beyond its well-intended use.\nA well conducted and robust study labelled as “fragile” is now interpreted with unreasonable scepticism.\nThe emotional loading inherent in the term “fragility” does a disservice to clinicians, scientists and patients who opted to participate in research.\n\nIn the context of RCTs, where power is set a priori, the fragility index is an unhelpful metric that casts robust findings into disrepute.\nA growing number of position papers are now proposing the use of the fragility index to be published alongside RCTs [@walsh2014; @tignanelli2019; @narayan2018; @evaniew2015].\nThis position should be respectfully challenged as a misinterpretation of statistical inference.\nThe fragility index represents a danger as to how good quality RCTs are interpreted.\nIt serves only to add unnecessary scepticism to legitimate results, and potentially undermines the scientific process.\nUncertainty in science should be embraced, but the debate should be directed toward where optimal decision boundaries exist, and not to re-imaginings of the p value.\n\n## Acknowledgements\n\nMany thanks to Giampiero Marra and Mervyn Singer (who provided valuable feedback to shape the first draft of this editorial), and Steve Harris, David Palmer, Dave Brealey, Clare Black and Ben Post.\n\n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}